---
title: "Coding1"
author: "Mengjia Zeng"
date: "2021/8/30"
output: html_document
---

## Generate Centers

```{r}
set.seed(6199)
p = 2;      
csize = 10;     # number of centers
sigma = 1;      # sd for generating the centers 
m1 = matrix(rnorm(csize*p), csize, p)*sigma + 
  cbind(rep(1, csize), rep(0, csize))
m0 = matrix(rnorm(csize*p), csize, p)*sigma + 
  cbind(rep(0, csize), rep(1, csize))
s = sqrt(1/5)
```

## Generate Data

```{r}
sim_params = list(
 csize = 10,      # number of centers
 p = 2,           # dimension
 s = sqrt(1/5),   # standard deviation for generating data
 n = 100,         # training size per class
 N = 5000,        # test size per class
 m0 = m0,         # 10 centers for class 0
 m1 = m1         # 10 centers for class 1
)
generate_sim_data = function(sim_params){
  p = sim_params$p
  s = sim_params$s 
  n = sim_params$n 
  N = sim_params$N 
  m1 = sim_params$m1 
  m0 = sim_params$m0
  csize = sim_params$csize
  
  id1 = sample(1:csize, n, replace = TRUE);
  id0 = sample(1:csize, n, replace = TRUE);
  traindata = matrix(rnorm(2*n*p), 2*n, p)*s + rbind(m1[id1,], m0[id0,])
  Ytrain = factor(c(rep(1,n), rep(0,n)))
  shuffle_row_id = sample(1:n)
  id1 = sample(1:csize, N, replace=TRUE);
  id0 = sample(1:csize, N, replace=TRUE); 
  testdata = matrix(rnorm(2*N*p), 2*N, p)*s + rbind(m1[id1,], m0[id0,])
  Ytest = factor(c(rep(1,N), rep(0,N)))
  
  # Return the training/test data along with labels
  list(
  traindata = traindata,
  Ytrain = Ytrain,
  testdata = testdata,
  Ytest = Ytest
  )
}
```

## Visualize Data

```{r}
mydata = generate_sim_data(sim_params)
traindata = mydata$train
Ytrain = mydata$Ytrain
testdata = mydata$testdata
Ytest = mydata$Ytest
n = nrow(traindata)

mycol = rep("blue", n)
mycol[Ytrain==0] = "red"
plot(traindata[, 1], traindata[, 2], type = "n", xlab = "", ylab = "")
points(traindata[, 1], traindata[, 2], col = mycol);
points(m1[, 1], m1[, 2], pch = "+", cex = 2, col = "blue");
points(m0[, 1], m0[, 2], pch = "+", cex = 2, col = "red"); 
legend("bottomright", pch = c(1,1), col = c("blue", "red"), 
       legend = c("class 1", "class 0"))  
```

## Wite KNN without a package

```{r}
N = nrow(testdata)
Ytrain0 = as.numeric(Ytrain)-1
myList <- vector(mode = "list", length = N) 
knn_own <- function(data, k){
  for (j in 1:N) {
    distance = numeric(n)
    for (i in 1:n){
      distance[i] = sqrt((data[j,1]-traindata[i,1])^2+(data[j,2]-traindata[i,2])^2)
      order_index = sort(distance, decreasing = FALSE, index.return=TRUE)$ix;
      if (max(which(distance == distance [k])) > k) {
        knn_index = order_index[1: max(which(distance == distance[k]))] #This code is used to break the distance ties. If there are some equal distances within the target interval, then this code will include all the index that are equal to the kth shortest distance. For example, if k=5 while the distance with k=6 is equal to k=5, then this code will include k=6 as well, meaning that this code will use k=6 instead of k=5.
        if (sum(Ytrain0[knn_index])>0.5*k) {
          myList[[j]] = 1
        } else if (sum(Ytrain0[knn_index]) < 0.5*k) {
          myList[[j]] = 0
        }
        else  {myList[[j]] = sample(0:1, 1)}
      }
      else {knn_index = order_index[1:k]
      if (sum(Ytrain0[knn_index])>0.5*k) {
        myList[[j]] = 1
      } else if (sum(Ytrain0[knn_index]) < 0.5*k) {
        myList[[j]] = 0
      }
      else  {myList[[j]] = sample(0:1, 1)} #This code is used to break the voting ties. This may occur when k is even. And this code is used to choose randomly between 0 and 1 when the probability of 0 and 1 is equal.
      }
      
    }
  }
  myList
}
```

## Use own function implementing KNN, k = 1

```{r}
test_pred_1 = knn_own(testdata, k=1)
test_pred_1 = unlist(test_pred_1)
test.err.knn = sum(Ytest != test_pred_1)/N
table(Ytest, test_pred_1)
```

## Use the KNN function from Package 'class', k = 1

```{r}
library(class)
test.pred1 = knn(traindata, testdata, Ytrain, k=1)
table(Ytest, test.pred1)
```

The result from my own code is the same as the result from knn when k=1.

## Use own function implementing KNN, k = 3

```{r}
test_pred_3 = knn_own(testdata, k=3)
test_pred_3 = unlist(test_pred_3)
test.err.knn = sum(Ytest != test_pred_3)/N
table(Ytest, test_pred_3)
```

## Use the KNN function from Package 'class', k = 3

```{r}
test.pred3 = knn(traindata, testdata, Ytrain, prob=TRUE, k=3)
table(Ytest, test.pred3)
```

## Buffer zone when k = 3 using KNN function

```{r}
which(test_pred_3 != test.pred3)
attributes(test.pred3)$prob[7572]
tmp = sweep(traindata, 2, testdata[7572,])
dist = rowSums(tmp^2)
cbind(dist, Ytrain)[order(dist)[1:7], ]
0.12046100*(1 + 0.0001)
```

When using KNN function from the package of 'class' with k = 3, there might be a buffer zone problem. We have run this code several times, sometimes, there is no difference between the knn function and our own knn function. However, somertimes, the 7572nd row may be different between the two functions. When we explored deeper into the KNN function, it turns out that the 3rd and 4th distance from this point are very close to each other, within the 3rd distance * (1 + 10^-4). And the probability in 7572nd point in testdata is 0.5. 


## Use own function implementing KNN, k = 5

```{r}
test_pred_5 = knn_own(testdata, k=5)
test_pred_5 = unlist(test_pred_5)
test.err.knn = sum(Ytest != test_pred_5)/N
table(Ytest, test_pred_5)
```

## Use the KNN function from Package 'class', k = 5

```{r}
test.pred5 = knn(traindata, testdata, Ytrain, k=5)
table(Ytest, test.pred5)
```

The result from my own code is the same as the result from knn when k=5.

## Part II

```{r}
# Function for Linear Regression Model
fit_LS_model = function(sim_data, verbose=FALSE) {
  
  # change Y from factor to numeric
  sim_data$Ytrain = as.numeric(sim_data$Ytrain) - 1
  sim_data$Ytest = as.numeric(sim_data$Ytest) - 1
  
  # fit a quadratic regression model
  model = lm(
    sim_data$Ytrain ~ 
      V1 + V2,
    as.data.frame(sim_data$traindata)
  )
  if (verbose) {
    print(summary(model))
  }
  decision_thresh = 0.5
  train_pred = as.numeric(model$fitted.values > decision_thresh)
  
  test_yhat = predict(
    model,
    newdata=as.data.frame(sim_data$testdata)
  )
  test_pred = as.numeric(test_yhat > decision_thresh)
  
  # return the mean classification errors on training/test sets
  list(
    train_error = sum(sim_data$Ytrain  != train_pred) / length(sim_data$Ytrain),
    test_error = sum(sim_data$Ytest  != test_pred) / 
      length(sim_data$Ytest)
  )
}
```


```{r}
# Function for Quadratic Regression Model
fit_qr_model = function(sim_data, verbose=FALSE) {
  
  # change Y from factor to numeric
  sim_data$Ytrain = as.numeric(sim_data$Ytrain) - 1
  sim_data$Ytest = as.numeric(sim_data$Ytest) - 1
  
  # fit a quadratic regression model
  model = lm(
    sim_data$Ytrain ~ 
      V1 + V2 + I(V1^2) + I(V2^2) + V1:V2,
    as.data.frame(sim_data$traindata)
  )
  if (verbose) {
    print(summary(model))
  }
  
  decision_thresh = 0.5
  train_pred = as.numeric(model$fitted.values > decision_thresh)
  
  test_yhat = predict(
    model,
    newdata=as.data.frame(sim_data$testdata)
  )
  test_pred = as.numeric(test_yhat > decision_thresh)
  
  # return the mean classification errors on training/test sets
  list(
    train_error = sum(sim_data$Ytrain  != train_pred) / length(sim_data$Ytrain),
    test_error = sum(sim_data$Ytest  != test_pred) / 
      length(sim_data$Ytest)
  )
}
```

```{r}
# CV-Knn Function Code
cvKNNAveErrorRate = function (k, data, respdata, foldNum){
  n = nrow(data)
  error = 0
  foldSize = floor(n/foldNum)
  myIndex = sample(1: n)
  for(runId in 1:foldNum){  
    testSetIndex = ((runId-1)*foldSize + 1):(ifelse(runId == foldNum, n, runId*foldSize))
    testSetIndex = myIndex[testSetIndex]
    trainX = data[-testSetIndex, ]
    trainY = respdata[-testSetIndex]
    testX = data[testSetIndex, ]
    testY = respdata[testSetIndex]
    predictY = knn(trainX, testX, trainY, k)
    error = error + sum(predictY != testY) 
  }
  list(
    error = error/n
  )
}
```


```{r}
# CV-Knn Function Code
cvKNN = function(data, respdata, foldNum) {
  n = nrow(data)
  foldSize = floor(n/foldNum)  
  KVector = seq(1, (nrow(data) - foldSize), 1)
  cvErrorRates = sapply(KVector, cvKNNAveErrorRate, data, respdata, foldNum)
  cvErrorRates = unlist(cvErrorRates)
  result = list()
  result$bestK = max(KVector[cvErrorRates == min(cvErrorRates)])
  result$cvError = cvErrorRates[KVector == result$bestK]
  result
}
```

```{r}
#Function to Calculate Bayes Rule based on the threshold of 1.
mixnorm = function(x, centers0, centers1, s){
  ## return the density ratio for a point x, where each 
  ## density is a mixture of normal with multiple components
  d1 = sum(exp(-apply((t(centers1) - x)^2, 2, sum) / (2 * s^2)))
  d0 = sum(exp(-apply((t(centers0) - x)^2, 2, sum) / (2 * s^2)))
  
  return (d1 / d0)
}
```

```{r}
# Generate 50 datasets and Compute with the linear regression model and quadratic regression model
mynewList <- vector(mode = "list", length = 4)
train.err.LS = numeric(50)
test.err.LS = numeric(50)
train.err.qr = numeric(50)
test.err.qr = numeric(50)
bestK = numeric(50)
train.err.cv = numeric(50)
test.err.cv = numeric(50)
train.err.bayes = numeric(50)
test.err.bayes = numeric(50)
for (i in 1: 50){
   mynewList[[i]] = generate_sim_data(sim_params)
   train.err.LS[i] = fit_LS_model(mynewList[[i]])$train_error
   test.err.LS[i] = fit_LS_model(mynewList[[i]])$test_error
   
   train.err.qr[[i]] = fit_qr_model(mynewList[[i]])$train_error
   test.err.qr[i] = fit_qr_model(mynewList[[i]])$test_error
   
   bestK[i] = cvKNN(mynewList[[i]]$traindata, mynewList[[i]]$Ytrain, foldNum = 10)$bestK
  
   train.err.cv[i] = sum(mynewList[[i]]$Ytrain != knn(mynewList[[i]]$traindata, mynewList[[i]]$traindata, mynewList[[i]]$Ytrain, bestK[i]))/nrow(mynewList[[i]]$traindata)
  test.err.cv[i] = sum(mynewList[[i]]$Ytest != knn(mynewList[[i]]$traindata, mynewList[[i]]$testdata, mynewList[[i]]$Ytrain, bestK[i]))/nrow(mynewList[[i]]$testdata)
   
  Ytrain = as.numeric(mynewList[[i]]$Ytrain) - 1
  Ytest = as.numeric(mynewList[[i]]$Ytest) - 1
  
  train_pred = ifelse(apply(mynewList[[i]]$traindata, 1, mixnorm, m0, m1, s) > 1, 1, 0)
  test_pred = ifelse(apply(mynewList[[i]]$testdata, 1, mixnorm, m0, m1, s) > 1, 1, 0)
  
  train.err.bayes[i] = sum(Ytrain != train_pred)/length(Ytrain)
  test.err.bayes[i] = sum(Ytest != test_pred)/length(Ytest)
 }
```

```{r}
boxplot(train.err.LS, test.err.LS, train.err.qr, test.err.qr, train.err.cv, test.err.cv, train.err.bayes, test.err.bayes, col = c("green", "red", "green", "red", "green", "red", "green", "red"), names = c("LS", "LS", "QR", "QR", "CV", "CV", "BR", "BR"), xlab = "Different Procedures", ylab = "Error")
legend("topright",pch = c(1,1), col = c("green", "red"), legend = c("Training Error", "Testing Error"), cex=0.7)
```

```{r}
bestK
print(paste0("mean of besk 50 K value: ", mean(bestK))) 
# mean for 50 k values
print(paste0("standard error of best 50 K value: ", sd(bestK)))
# standard deviation for 50 k values
boxplot(bestK)
```

```{r}
#Here is my another try to calculate Bayes Rule, I think it also works. (Only based on the expression provided by the assignment.)
mixnorm = function(data, respdata) {
  n = nrow(data)
  d_ratio = vector(mode = "list", length = n)
  d1 = numeric(n)
  d0 = numeric(n)
  pred_Bayes = numeric(n)
  error = 0
  for (i in 1:n){
    d1_int = numeric(10)
    d0_int = numeric(10)
    for (j in 1:10) {
      d1_int[j] = exp(-sum((m1[j,] - data[i,])^2)/(2 * s^2))
      d0_int[j] = exp(-sum((m0[j,] - data[i,])^2)/(2 * s^2))
    }
    d1[i] = sum(d1_int)
    d0[i] = sum(d0_int)
    d_ratio[[i]] = d1[i]/d0[i]
    if (d_ratio[[i]]>1) {
      pred_Bayes[i] = 1
    } else {
      pred_Bayes[i] = 0
    }
  }
  list(
    error = error + sum(respdata != pred_Bayes)/n,
    d_ratio,
    pred_Bayes
  )
}
```

